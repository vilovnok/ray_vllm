# This file was generated using the `serve build` command on Ray v2.40.0.

proxy_location: EveryNode
http_options:
  host: 0.0.0.0
  port: 8000

grpc_options:
  port: 9000
  grpc_servicer_functions: []

logging_config:
  encoding: TEXT
  log_level: INFO
  logs_dir: null
  enable_access_log: true

applications:
  - name: app
    route_prefix: /
    import_path: src.vllm_serve:llm_app
    deployments:
      - name: VLLMDeployment
        runtime_env:
          env_vars:
            VLLM_USE_V1: "1"
            VLLM_LOGGING_LEVEL: "DEBUG"
            VLLM_DISABLE_COMPILE_CACHE: "1"
            VLLM_ATTENTION_BACKEND: "TRITON_ATTN"
        ray_actor_options:
          num_cpus: 4
          num_gpus: 2
        user_config:
          engine_args:
            model: Qwen/Qwen2.5-1.5B-Instruct
            served_model_name: qwen
            trust_remote_code: true
            tensor_parallel_size: 1
            max_model_len: 32000
            max_num_seqs: 16
            gpu_memory_utilization: 0.9
            max_num_batched_tokens: 512
            enforce_eager: true